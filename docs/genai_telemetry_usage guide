#!/usr/bin/env python3
"""
================================================================================
GenAI Telemetry - OpenAI â†’ Splunk Dashboard
================================================================================

All data automatically flows to Splunk Dashboard.
No print statements needed - decorators handle everything.

Author: Kamal Singh Bisht
License: Apache 2.0
================================================================================
"""

# =============================================================================
# INSTALLATION
# =============================================================================
# pip install genai-telemetry openai


# =============================================================================
# SETUP (Once at app startup)
# =============================================================================

from genai_telemetry import (
    setup_telemetry,
    trace_llm,
    trace_embedding,
    trace_retrieval,
    trace_tool,
    trace_chain,
    trace_agent,
    get_telemetry
)
from openai import OpenAI

# Configure - data goes to Splunk automatically
setup_telemetry(
    workflow_name="my-chatbot",
    exporter="splunk",
    splunk_url="https://your-splunk:8088",
    splunk_token="your-hec-token",
    splunk_index="genai_traces"
)

client = OpenAI(api_key="your-openai-api-key")


# =============================================================================
# EXAMPLE 1: BASIC LLM CALL
# =============================================================================
# Decorator automatically sends to Splunk:
# - trace_id, span_id
# - model_name, model_provider
# - duration_ms
# - input_tokens, output_tokens
# - status (OK/ERROR)

@trace_llm(model_name="gpt-4o", model_provider="openai")
def chat(message: str) -> str:
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": message}]
    )
    return response.choices[0].message.content


# =============================================================================
# EXAMPLE 2: CHAT WITH SYSTEM PROMPT
# =============================================================================

@trace_llm(model_name="gpt-4o", model_provider="openai")
def chat_with_system(user_message: str, system_prompt: str) -> str:
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]
    )
    return response.choices[0].message.content


# =============================================================================
# EXAMPLE 3: EMBEDDINGS
# =============================================================================
# Sends to Splunk:
# - span_type: EMBEDDING
# - embedding_model
# - duration_ms

@trace_embedding(model="text-embedding-3-small")
def get_embedding(text: str) -> list:
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return response.data[0].embedding


# =============================================================================
# EXAMPLE 4: VECTOR SEARCH
# =============================================================================
# Sends to Splunk:
# - span_type: RETRIEVER
# - vector_store
# - documents_retrieved (count)
# - duration_ms

@trace_retrieval(vector_store="pinecone", embedding_model="text-embedding-3-small")
def search_documents(query_embedding: list, top_k: int = 5) -> list:
    # Your vector DB search here (Pinecone, Chroma, Weaviate, etc.)
    # index.query(vector=query_embedding, top_k=top_k)
    return [
        {"id": "doc1", "text": "Document 1 content", "score": 0.95},
        {"id": "doc2", "text": "Document 2 content", "score": 0.87},
    ]


# =============================================================================
# EXAMPLE 5: TOOL CALLS
# =============================================================================
# Sends to Splunk:
# - span_type: TOOL
# - tool_name
# - duration_ms

@trace_tool(tool_name="get_weather")
def get_weather(city: str) -> dict:
    # Your weather API call
    return {"city": city, "temperature": 72, "condition": "sunny"}

@trace_tool(tool_name="search_database")
def search_database(query: str) -> list:
    # Your database query
    return [{"id": 1, "data": "result"}]

@trace_tool(tool_name="calculator")
def calculator(expression: str) -> float:
    return eval(expression)


# =============================================================================
# EXAMPLE 6: COMPLETE RAG PIPELINE
# =============================================================================
# trace_chain creates NEW trace_id and groups all child spans

@trace_chain(name="rag-pipeline")
def rag_query(question: str) -> str:
    # Step 1: Embed question
    query_embedding = get_embedding(question)
    
    # Step 2: Search documents
    docs = search_documents(query_embedding, top_k=5)
    context = "\n".join([doc["text"] for doc in docs])
    
    # Step 3: Generate answer
    answer = generate_answer(context, question)
    
    return answer

@trace_llm(model_name="gpt-4o", model_provider="openai")
def generate_answer(context: str, question: str) -> str:
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": f"Answer based on context:\n{context}"},
            {"role": "user", "content": question}
        ]
    )
    return response.choices[0].message.content

# Calling rag_query() sends 4 spans to Splunk:
# 1. CHAIN: rag-pipeline (parent)
# 2. EMBEDDING: get_embedding
# 3. RETRIEVER: search_documents
# 4. LLM: generate_answer


# =============================================================================
# EXAMPLE 7: AGENT WITH TOOLS
# =============================================================================

import json

@trace_agent(agent_name="assistant-agent", agent_type="function-calling")
def agent_with_tools(message: str) -> str:
    tools = [
        {
            "type": "function",
            "function": {
                "name": "get_weather",
                "description": "Get weather for a city",
                "parameters": {
                    "type": "object",
                    "properties": {"city": {"type": "string"}},
                    "required": ["city"]
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "calculator",
                "description": "Calculate math expression",
                "parameters": {
                    "type": "object",
                    "properties": {"expression": {"type": "string"}},
                    "required": ["expression"]
                }
            }
        }
    ]
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": message}],
        tools=tools,
        tool_choice="auto"
    )
    
    if response.choices[0].message.tool_calls:
        tool_call = response.choices[0].message.tool_calls[0]
        func_name = tool_call.function.name
        args = json.loads(tool_call.function.arguments)
        
        # Execute tool (each is traced)
        if func_name == "get_weather":
            result = get_weather(**args)
        elif func_name == "calculator":
            result = calculator(**args)
        
        # Get final response
        messages = [
            {"role": "user", "content": message},
            response.choices[0].message,
            {"role": "tool", "tool_call_id": tool_call.id, "content": json.dumps(result)}
        ]
        
        final = client.chat.completions.create(model="gpt-4o", messages=messages)
        return final.choices[0].message.content
    
    return response.choices[0].message.content


# =============================================================================
# EXAMPLE 8: MULTIPLE MODELS
# =============================================================================

@trace_llm(model_name="gpt-4o", model_provider="openai")
def chat_gpt4o(message: str) -> str:
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": message}]
    )
    return response.choices[0].message.content

@trace_llm(model_name="gpt-4o-mini", model_provider="openai")
def chat_gpt4o_mini(message: str) -> str:
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": message}]
    )
    return response.choices[0].message.content

@trace_llm(model_name="gpt-4-turbo", model_provider="openai")
def chat_gpt4_turbo(message: str) -> str:
    response = client.chat.completions.create(
        model="gpt-4-turbo",
        messages=[{"role": "user", "content": message}]
    )
    return response.choices[0].message.content


# =============================================================================
# EXAMPLE 9: CUSTOM ATTRIBUTES (Manual Span)
# =============================================================================

def process_with_custom_data(message: str, user_id: str, session_id: str) -> str:
    telemetry = get_telemetry()
    
    with telemetry.start_span("custom_process", "LLM") as span:
        # Add custom attributes
        span.set_attribute("user_id", user_id)
        span.set_attribute("session_id", session_id)
        span.set_attribute("message_length", len(message))
        
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": message}]
        )
        
        # Add more attributes after response
        span.input_tokens = response.usage.prompt_tokens
        span.output_tokens = response.usage.completion_tokens
        span.model_name = "gpt-4o"
        
        return response.choices[0].message.content

# Sends to Splunk:
# - All standard fields
# - user_id, session_id, message_length (custom)


# =============================================================================
# EXAMPLE 10: SEND SPAN DIRECTLY (No Decorator)
# =============================================================================

def manual_tracking(message: str) -> str:
    telemetry = get_telemetry()
    
    import time
    start = time.time()
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": message}]
    )
    
    duration_ms = (time.time() - start) * 1000
    
    # Send span directly to Splunk
    telemetry.send_span(
        span_type="LLM",
        name="manual_chat",
        model_name="gpt-4o",
        model_provider="openai",
        duration_ms=duration_ms,
        input_tokens=response.usage.prompt_tokens,
        output_tokens=response.usage.completion_tokens,
        # Add any custom fields
        custom_field="custom_value",
        user_tier="premium"
    )
    
    return response.choices[0].message.content


# =============================================================================
# EXAMPLE 11: BATCH PROCESSING
# =============================================================================

@trace_chain(name="batch-processing")
def process_batch(questions: list) -> list:
    results = []
    for question in questions:
        answer = chat(question)
        results.append({"question": question, "answer": answer})
    return results

# Each chat() call sends its own span
# All grouped under one trace_id from @trace_chain


# =============================================================================
# EXAMPLE 12: ERROR HANDLING (Errors auto-sent to Splunk)
# =============================================================================

@trace_llm(model_name="gpt-4o", model_provider="openai")
def chat_with_error_handling(message: str) -> str:
    # If this fails, error details are automatically sent to Splunk:
    # - status: "ERROR"
    # - is_error: 1
    # - error_message: "..."
    # - error_type: "APIError"
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": message}]
    )
    return response.choices[0].message.content


# =============================================================================
# EXAMPLE 13: CONVERSATION (Multi-turn)
# =============================================================================

@trace_chain(name="conversation")
def multi_turn_chat(conversation_history: list, new_message: str) -> str:
    conversation_history.append({"role": "user", "content": new_message})
    
    response = chat_continue(conversation_history)
    
    conversation_history.append({"role": "assistant", "content": response})
    
    return response

@trace_llm(model_name="gpt-4o", model_provider="openai")
def chat_continue(messages: list) -> str:
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages
    )
    return response.choices[0].message.content


# =============================================================================
# USAGE - Just call your functions, data goes to Splunk
# =============================================================================

# Basic chat
answer = chat("What is Python?")

# RAG pipeline
answer = rag_query("Explain machine learning")

# Agent with tools
answer = agent_with_tools("What's the weather in Paris?")

# Batch processing
results = process_batch(["Question 1", "Question 2", "Question 3"])

# All data is now in Splunk Dashboard:
# - genai_traces index
# - Fields: span_type, model_name, duration_ms, input_tokens, output_tokens, etc.


# =============================================================================
# SPLUNK SEARCHES FOR DASHBOARD
# =============================================================================
"""
# Total LLM Calls
index=genai_traces span_type="LLM" | stats count

# Token Usage
index=genai_traces span_type="LLM" 
| stats sum(input_tokens) as input, sum(output_tokens) as output

# Average Latency by Model
index=genai_traces span_type="LLM" 
| stats avg(duration_ms) as avg_latency by model_name

# Error Rate
index=genai_traces span_type="LLM" 
| stats count(eval(is_error=1)) as errors, count as total 
| eval error_rate=round((errors/total)*100, 2)

# Calls Over Time
index=genai_traces span_type="LLM" 
| timechart span=1h count by model_name

# Top Workflows
index=genai_traces 
| stats count by workflow_name 
| sort -count

# Cost Estimation (example rates)
index=genai_traces span_type="LLM" 
| eval cost=case(
    model_name="gpt-4o", (input_tokens*0.005 + output_tokens*0.015)/1000,
    model_name="gpt-4o-mini", (input_tokens*0.00015 + output_tokens*0.0006)/1000,
    1=1, 0
)
| stats sum(cost) as total_cost by model_name
"""
